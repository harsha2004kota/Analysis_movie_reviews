# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oIaR-9999WiPK9ynUScLmAtvQbApgfr7
"""

import pandas as pd
df=pd.read_csv("/content/IMDB Dataset.csv")

df.head()

df.tail()

from google.colab import drive
drive.mount('/content/drive')

df.shape

df.describe()

df.info()

df.isnull().sum()

df.columns

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

#Text Cleaning and Preprocessing
swords = {
    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',
    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',
    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',
    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',
    'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',
    'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',
    'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',
    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',
    'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'
}
ps = PorterStemmer()

def preprocess_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    # Remove non-alphabetic characters
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    # Convert to lowercase
    text = text.lower()
    # Tokenize
    words = text.split()
    # Remove stopwords and stem the words
    words = [ps.stem(word) for word in words if word not in swords]
    return ' '.join(words)

df['cleanreview'] = df['review'].apply(preprocess_text)

# Display the first few cleaned reviews
df[['review', 'cleanreview']].head()

sns.countplot(x='sentiment', data=df)
plt.title('Sentiment')
plt.show()

df['review_length'] = df['cleanreview'].apply(lambda x: len(x.split()))
sns.histplot(df['review_length'], bins=50, kde=True)
plt.title('Review Length Distribution')
plt.show()

#NLP Techniques
vectorizer = CountVectorizer(max_features=5000)
X_bow = vectorizer.fit_transform(df['cleanreview'])
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf_vectorizer.fit_transform(df['cleanreview'])

#Sentiment classification model
y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df['cleaned_review'])
X_seq = tokenizer.texts_to_sequences(df['cleaned_review'])
X_padded = pad_sequences(X_seq, maxlen=500)

X_train_lstm, X_test_lstm, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)
X_train2_lstm, X_val_lstm, y_train2, y_val = train_test_split(X_train_lstm, y_train, test_size=0.4, random_state=42)

lstm_model = Sequential()
lstm_model.add(Embedding(5000, 128, input_length=500))
lstm_model.add(SpatialDropout1D(0.2))
lstm_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
lstm_model.add(Dense(1, activation='sigmoid'))
lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = lstm_model.fit(X_train2_lstm, y_train2, epochs=1, batch_size=256, validation_data=(X_val_lstm, y_val))

#Model Evaluation
def evaluate_model(y_test, y_pred):
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    return accuracy, precision, recall, f1
y_pred_lstm = (lstm_model.predict(X_test_lstm) > 0.5).astype("int32")
acc_lstm, prec_lstm, rec_lstm, f1_lstm = evaluate_model(y_test, y_pred_lstm)
print(f'LSTM - Accuracy: {acc_lstm}, Precision: {prec_lstm}, Recall: {rec_lstm}, F1 Score: {f1_lstm}')

